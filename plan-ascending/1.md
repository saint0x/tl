# Phase 1: Foundation & Core Storage âœ… COMPLETE

**Status**: 72 tests passing (67 unit + 5 integration)

## Workspace Setup

- [x] Update root `Cargo.toml` with workspace definition
  - [x] Define workspace members: `core`, `watcher`, `journal`, `jj`, `cli`
  - [x] Define `[workspace.dependencies]` for version consistency
  - [x] Add blake3, memmap2, sled, tokio, parking_lot, crossbeam-channel, dashmap
  - [x] Add smallvec, ahash, bytes, serde, bincode
  - [x] Add notify, clap, tracing, jj-lib, chrono, ulid, thiserror, anyhow

- [x] Create crate directory structure
  - [x] `mkdir -p crates/core/src`
  - [x] `mkdir -p crates/watcher/src`
  - [x] `mkdir -p crates/journal/src`
  - [x] `mkdir -p crates/jj/src`
  - [x] `mkdir -p crates/cli/src`
  - [x] `mkdir -p benches`

## core: Hash Module (hash.rs)

- [x] Create `crates/core/Cargo.toml`
  - [x] Add workspace dependency references
  - [x] Core deps: blake3, memmap2, parking_lot, dashmap, smallvec, ahash, bytes, serde, bincode

- [x] Implement `hash.rs`: BLAKE3 wrapper
  - [x] `Blake3Hash` type: `struct Blake3Hash([u8; 32])`
  - [x] Implement `Copy`, `Clone`, `Hash`, `Eq`, `PartialEq`, `Debug`
  - [x] `hash_bytes(data: &[u8]) -> Blake3Hash` - basic hashing
  - [x] `hash_file(path: &Path) -> Result<Blake3Hash>` - streaming hash for files
  - [x] `hash_file_mmap(path: &Path) -> Result<Blake3Hash>` - memory-mapped for large files (>4MB)
  - [x] `IncrementalHasher` wrapper around `blake3::Hasher`
  - [x] Hex encoding/decoding: `to_hex()`, `from_hex()`
  - [x] Parallel hashing support using rayon for multiple files

## core: Blob Module (blob.rs)

- [x] Define blob header format
  ```rust
  struct BlobHeaderV1 {
      magic: [u8; 4],     // "SNB1"
      flags: u8,          // compression, type
      orig_len: u64,      // original size
      stored_len: u64,    // compressed size (if compressed)
  }
  ```
  - [x] Flag bits: bit0=compressed (zstd), bit1-7=reserved
  - [x] Implement header serialization/deserialization

- [x] Implement `Blob` type
  - [x] `struct Blob { hash: Blake3Hash, size: u64, compressed: bool }`
  - [x] `from_bytes(data: &[u8]) -> Result<(Blob, Vec<u8>)>` - create blob from bytes
  - [x] `to_bytes(&self, data: &[u8]) -> Result<Vec<u8>>` - serialize with header
  - [x] Compression: zstd level 3 for blobs > 4KB
  - [x] Decompression on read

- [x] Implement `BlobStore`
  - [x] `DashMap<Blake3Hash, Arc<Blob>>` for in-memory cache
  - [x] `BufferPool<BytesMut>` - pre-allocated 64KB buffers (object pool pattern)
  - [x] `write_blob(hash: Blake3Hash, data: &[u8]) -> Result<()>`
    - [x] Atomic write: write to `.tl/tmp/ingest/<uuid>` then rename to `objects/blobs/<hh>/<rest>`
    - [x] First 2 hex chars as subdirectory (reduce inode pressure)
  - [x] `read_blob(hash: Blake3Hash) -> Result<Vec<u8>>`
    - [x] Check cache first
    - [x] Memory-map for large blobs (zero-copy)
    - [x] Decompress if needed
  - [x] `has_blob(hash: Blake3Hash) -> bool` - check existence without reading
  - [x] LRU eviction policy (configurable max cache size, default 50MB)

- [x] Memory optimization
  - [x] Pre-allocate buffer pool: 16 buffers of 64KB each
  - [x] Path interning: use `Arc<Path>` for blob paths
  - [x] Lazy loading: only load metadata on cache miss

## core: Tree Module (tree.rs)

- [x] Define tree entry types
  ```rust
  enum EntryKind {
      File,
      Symlink,
      Submodule,  // optional for MVP
  }

  struct Entry {
      kind: EntryKind,
      mode: u32,           // Unix permission bits
      blob_hash: Blake3Hash,
  }
  ```

- [x] Define tree structure (flat for MVP)
  ```rust
  struct Tree {
      entries: FxHashMap<SmallVec<[u8; 64]>, Entry>,  // path -> entry
  }
  ```
  - [x] Use `FxHashMap` (faster for small keys)
  - [x] `SmallVec` for paths (stack allocation for paths < 64 bytes)

- [x] Implement tree serialization (TreeV1 format)
  ```
  TreeV1:
    magic: [u8; 4] = "SNT1"
    entry_count: u32
    repeated entry:
      path_len: u16
      path_bytes: [u8; path_len]
      kind: u8              // 0=file, 1=symlink, 2=submodule
      mode: u32
      blob_hash: [u8; 32]
  ```
  - [x] Entries must be sorted lexicographically by path (deterministic hash)
  - [x] `serialize() -> Vec<u8>` - encode to bytes
  - [x] `deserialize(bytes: &[u8]) -> Result<Tree>` - decode from bytes

- [x] Implement tree hashing
  - [x] `hash(&self) -> Blake3Hash` - hash serialized tree bytes
  - [x] Tree hash is deterministic (same content = same hash)

- [x] Implement tree diffing
  - [x] `diff(old: &Tree, new: &Tree) -> TreeDiff`
  - [x] `TreeDiff` contains: added, removed, modified entries
  - [x] Efficient: only compare changed subtrees (use hash equality)

- [x] Implement incremental tree update
  - [x] `update_entries(base: &Tree, changes: Vec<(Path, Option<Entry>)>) -> Tree`
  - [x] Apply changes to base tree (insert/remove entries)
  - [x] Return new tree without full rescan

## core: Store Module (store.rs)

- [x] Define `.tl/` directory layout
  ```
  .tl/
    config.toml
    HEAD
    locks/
      daemon.lock
      gc.lock
    journal/
      ops.log
      ops.log.idx
    objects/
      blobs/
      trees/
    refs/
      pins/
      heads/
    state/
      pathmap.bin
      watcher.state
      metrics.json
    tmp/
      ingest/
      gc/
  ```

- [x] Implement `Store` initialization
  - [x] `init(repo_root: &Path) -> Result<Store>` - create `.tl/` structure
  - [x] Create all subdirectories
  - [x] Initialize `config.toml` with defaults
  - [x] Initialize empty `ops.log`

- [x] Implement `Store` struct
  - [x] `struct Store { root: PathBuf, blob_store: BlobStore, tree_cache: DashMap<Blake3Hash, Arc<Tree>> }`
  - [x] `open(repo_root: &Path) -> Result<Store>` - open existing store
  - [x] Validate `.tl/` structure on open

- [x] Implement tree storage
  - [x] `write_tree(tree: &Tree) -> Result<Blake3Hash>`
    - [x] Serialize tree
    - [x] Compute hash
    - [x] Write to `objects/trees/<hh>/<rest>`
    - [x] Atomic write (tmp + rename)
  - [x] `read_tree(hash: Blake3Hash) -> Result<Tree>`
    - [x] Check cache first
    - [x] Read from disk + deserialize
    - [x] Cache result

- [x] Implement atomic write helpers
  - [x] `atomic_write(tmp_dir: &Path, target: &Path, data: &[u8]) -> Result<()>`
    - [x] Write to temp file in tmp_dir
    - [x] Fsync temp file
    - [x] Rename to target
    - [x] Fsync parent directory (crash safety)

- [x] Path normalization utilities
  - [x] `normalize_path(path: &Path) -> Result<PathBuf>`
    - [x] Convert to relative path with `/` separator
    - [x] Reject `..` and absolute paths
    - [x] Remove `./` prefix

- [x] Ignore `.tl/` and `.git/` helpers
  - [x] `should_ignore(path: &Path) -> bool`
  - [x] Never watch or checkpoint `.tl/` or `.git/`

## Testing

- [x] Unit tests for hash.rs
  - [x] Test hash consistency (same input = same hash)
  - [x] Test hex encoding/decoding round-trip
  - [x] Test incremental hasher

- [x] Unit tests for blob.rs
  - [x] Test blob serialization/deserialization
  - [x] Test compression/decompression
  - [x] Test buffer pool reuse
  - [x] Test atomic writes

- [x] Unit tests for tree.rs
  - [x] Test deterministic serialization (sorted entries)
  - [x] Test tree hashing (same tree = same hash)
  - [x] Test tree diffing
  - [x] Test incremental update

- [x] Unit tests for store.rs
  - [x] Test store initialization
  - [x] Test atomic write operations
  - [x] Test path normalization
  - [x] Test ignore rules

## Memory Profiling Baseline

- [x] Create benchmark for blob operations
  - [x] Measure memory usage for 1000 blob writes
  - [x] Measure buffer pool efficiency
  - [x] Target: < 10MB for idle state

- [x] Create benchmark for tree operations
  - [x] Measure memory for 10k-entry tree
  - [x] Measure incremental update performance
  - [x] Target: < 5ms for small updates (1-5 files)
